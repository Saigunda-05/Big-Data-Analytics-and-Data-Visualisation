# -*- coding: utf-8 -*-
"""SJ-245B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UaMUwFpN6o-Blzcx-bNmxo30NajlFudj

# Setup PySpark
"""

# Installing Java, Spark, and Findspark
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz
!pip install -q findspark

# Setting environment variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop3.2"

# Initializing Spark
import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

"""# Loading and Explore the Data"""

from pyspark.sql.functions import *
from pyspark.sql.types import *
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.linalg import Vectors
from pyspark.sql.functions import *
from pyspark.sql.window import Window
from pyspark.sql.functions import mean, stddev
from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder
from pyspark.ml import Pipeline
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import RandomForestRegressor

import warnings
warnings.filterwarnings('ignore')

# Loading the dataset with correct schema
df = spark.read.csv('/content/nyc_taxi_trip_duration.csv', header=True, inferSchema=True)

# schema
print("Schema:")
df.printSchema()

print("\nSample data:")
df.show(5)

"""# Data Cleaning and Preprocessing"""

# Checking for null values
print("Null values count:")
df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()

# Calculating the actual trip duration from timestamps for validation
df = df.withColumn("calculated_duration",
                  (col("dropoff_datetime").cast("long") - col("pickup_datetime").cast("long")))

# Checking if provided duration matches calculated duration
df.select("trip_duration", "calculated_duration").show()

# Calculating distance using Haversine formula
def haversine(lon1, lat1, lon2, lat2):
    # Converting degrees to radians
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])

    # Haversine formula
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    r = 6371  # Radius of earth in kilometers
    return c * r

# Register the UDF
haversine_udf = udf(haversine, DoubleType())

from pyspark.sql.functions import radians, sin, cos, sqrt, atan2, col, hour, dayofmonth, month, dayofweek, to_timestamp

# Converting datetime columns to datetime type
df = df.withColumn("pickup_datetime", to_timestamp("pickup_datetime"))
df = df.withColumn("dropoff_datetime", to_timestamp("dropoff_datetime"))

# Calculating distance using Spark SQL functions (Haversine formula)
R = 6371  # Radius of Earth in kilometers

lat1_rad = radians(col("pickup_latitude"))
lon1_rad = radians(col("pickup_longitude"))
lat2_rad = radians(col("dropoff_latitude"))
lon2_rad = radians(col("dropoff_longitude"))

dlon = lon2_rad - lon1_rad
dlat = lat2_rad - lat1_rad

a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2
c = 2 * atan2(sqrt(a), sqrt(1 - a))

distance = R * c

df = df.withColumn("distance", distance)

# Extracting time features
df = df.withColumn("pickup_hour", hour(col("pickup_datetime")))
df = df.withColumn("pickup_day", dayofmonth(col("pickup_datetime")))
df = df.withColumn("pickup_month", month(col("pickup_datetime")))
df = df.withColumn("pickup_day_of_week", dayofweek(col("pickup_datetime")))

# Showing the enhanced dataframe
df.show()

"""# Exploratory Data Analysis (EDA)"""

passenger_count_counts = pdf_filtered['passenger_count'].value_counts().sort_index()

plt.figure(figsize=(10, 6))
sns.barplot(x=passenger_count_counts.index, y=passenger_count_counts.values)
plt.title('Distribution of Passenger Count')
plt.xlabel('Passenger Count')
plt.ylabel('Frequency')
plt.show()

trip_count_by_hour = pdf_filtered.groupby('pickup_hour').size()


plt.figure(figsize=(12, 7))
sns.lineplot(x=trip_count_by_hour.index, y=trip_count_by_hour.values)
plt.title('Trip Frequency by Hour of Day (Line Plot)')
plt.xlabel('Hour of Day')
plt.ylabel('Number of Trips')
plt.show()

trip_count_by_day_of_week = pdf_filtered.groupby('pickup_day_of_week').size()

# Map numerical day of the week to names
day_names = {1: 'Sunday', 2: 'Monday', 3: 'Tuesday', 4: 'Wednesday', 5: 'Thursday', 6: 'Friday', 7: 'Saturday'}
trip_count_by_day_of_week = trip_count_by_day_of_week.rename(index=day_names)

plt.figure(figsize=(12, 7))
sns.barplot(x=trip_count_by_day_of_week.index, y=trip_count_by_day_of_week.values, order=day_names.values())
plt.title('Trip Frequency by Day of Week')
plt.xlabel('Day of Week')
plt.ylabel('Number of Trips')
plt.show()

trip_count_by_month = pdf_filtered.groupby('pickup_month').size()

# Map numerical month to names
month_names = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun'}
trip_count_by_month = trip_count_by_month.rename(index=month_names)

plt.figure(figsize=(12, 7))
sns.barplot(x=trip_count_by_month.index, y=trip_count_by_month.values, order=month_names.values())
plt.title('Trip Frequency by Month')
plt.xlabel('Month')
plt.ylabel('Number of Trips')
plt.show()

"""# Outlier Removal"""

# Calculating mean and stddev
stats = df.select(
    mean("trip_duration").alias("mean"),
    stddev("trip_duration").alias("stddev")
).collect()

mean_val = stats[0]["mean"]
stddev_val = stats[0]["stddev"]

# Defining outlier thresholds (3 standard deviations)
lower_bound = mean_val - 3 * stddev_val
upper_bound = mean_val + 3 * stddev_val

print(f"Mean: {mean_val}, StdDev: {stddev_val}")
print(f"Lower bound: {lower_bound}, Upper bound: {upper_bound}")

# Filter outliers
df_clean = df.filter(
    (col("trip_duration") > lower_bound) &
    (col("trip_duration") < upper_bound)
)

print(f"Original count: {df.count()}, Cleaned count: {df_clean.count()}")

"""# Feature Engineering and Model Preparation"""

# Preparing features
feature_cols = ['vendor_id', 'passenger_count', 'distance',
                'pickup_hour', 'pickup_day_of_week']

# Assemble features
assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="features"
)

# Splitting data into train and test sets
train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)

print(f"Training data count: {train_data.count()}")
print(f"Test data count: {test_data.count()}")

"""# Model Building - Linear Regression"""

# Creating Linear Regression model
lr = LinearRegression(
    featuresCol="features",
    labelCol="trip_duration",
    predictionCol="prediction",
    maxIter=10,
    regParam=0.3,
    elasticNetParam=0.8
)

# Creating pipeline
pipeline_lr = Pipeline(stages=[assembler, lr])

# Training model
model_lr = pipeline_lr.fit(train_data)

# Making predictions
predictions_lr = model_lr.transform(test_data)

# Evaluating model
evaluator = RegressionEvaluator(
    labelCol="trip_duration",
    predictionCol="prediction",
    metricName="rmse"
)

rmse_lr = evaluator.evaluate(predictions_lr)
print(f"Root Mean Squared Error (RMSE) for Linear Regression: {rmse_lr}")

# Showing some predictions
predictions_lr.select("trip_duration", "prediction", "distance").show(5)

# Additional metrics
evaluator_r2 = RegressionEvaluator(
    labelCol="trip_duration",
    predictionCol="prediction",
    metricName="r2"
)

r2_lr = evaluator_r2.evaluate(predictions_lr)
print(f"R-squared for Linear Regression: {r2_lr}")

# Plot actual vs predicted
pdf_lr = predictions_lr.select("trip_duration", "prediction").toPandas()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='trip_duration', y='prediction', data=pdf_lr)
plt.plot([pdf_lr['trip_duration'].min(), pdf_lr['trip_duration'].max()],
         [pdf_lr['trip_duration'].min(), pdf_lr['trip_duration'].max()],
         'r--')
plt.title('Actual vs Predicted Trip Duration (Linear Regression)')
plt.xlabel('Actual Duration (seconds)')
plt.ylabel('Predicted Duration (seconds)')
plt.show()

"""# Model Building - Random Forest Regression"""

# Creating Random Forest model
rf = RandomForestRegressor(
    featuresCol="features",
    labelCol="trip_duration",
    predictionCol="prediction",
    numTrees=20,
    maxDepth=5,
    seed=42
)

# Creating pipeline
pipeline_rf = Pipeline(stages=[assembler, rf])

# Training model
model_rf = pipeline_rf.fit(train_data)

# Making predictions
predictions_rf = model_rf.transform(test_data)

# Evaluating model
rmse_rf = evaluator.evaluate(predictions_rf)
print(f"Root Mean Squared Error (RMSE) for Random Forest: {rmse_rf}")

r2_rf = evaluator_r2.evaluate(predictions_rf)
print(f"R-squared for Random Forest: {r2_rf}")

# Showing some predictions
predictions_rf.select("trip_duration", "prediction", "distance").show(5)

# Plot actual vs predicted
pdf_rf = predictions_rf.select("trip_duration", "prediction").toPandas()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='trip_duration', y='prediction', data=pdf_rf)
plt.plot([pdf_rf['trip_duration'].min(), pdf_rf['trip_duration'].max()],
         [pdf_rf['trip_duration'].min(), pdf_rf['trip_duration'].max()],
         'r--')
plt.title('Actual vs Predicted Trip Duration (Random Forest)')
plt.xlabel('Actual Duration (seconds)')
plt.ylabel('Predicted Duration (seconds)')
plt.show()

# Feature importance
rf_model = model_rf.stages[-1]
importances = rf_model.featureImportances.toArray()
feature_importance = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': importances
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance)
plt.title('Feature Importance (Random Forest)')
plt.show()

"""# Model Comparison and Conclusion"""

# Comparing both models
results = {
    'Model': ['Linear Regression', 'Random Forest'],
    'RMSE': [rmse_lr, rmse_rf],
    'R-squared': [r2_lr, r2_rf]
}

results_df = pd.DataFrame(results)
print("\nModel Comparison:")
print(results_df)

# Visual comparison
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='RMSE', data=results_df)
plt.title('Model Comparison by RMSE')
plt.ylabel('Root Mean Squared Error')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='R-squared', data=results_df)
plt.title('Model Comparison by R-squared')
plt.ylabel('R-squared')
plt.ylim(0, 1)
plt.show()